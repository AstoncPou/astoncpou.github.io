<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_fw031qduy7rh-6>li:before{content:"\0025cf   "}ul.lst-kix_fw031qduy7rh-8{list-style-type:none}.lst-kix_fw031qduy7rh-5>li:before{content:"\0025a0   "}.lst-kix_fw031qduy7rh-7>li:before{content:"\0025cb   "}ul.lst-kix_x4rtdbz82adu-7{list-style-type:none}ul.lst-kix_x4rtdbz82adu-8{list-style-type:none}.lst-kix_fw031qduy7rh-8>li:before{content:"\0025a0   "}.lst-kix_t6bn5dv180ed-1>li:before{content:"\0025cb   "}.lst-kix_s3obbdkamyme-0>li:before{content:"\0025cf   "}ul.lst-kix_x4rtdbz82adu-0{list-style-type:none}ul.lst-kix_x4rtdbz82adu-1{list-style-type:none}ul.lst-kix_x4rtdbz82adu-2{list-style-type:none}ul.lst-kix_x4rtdbz82adu-3{list-style-type:none}ul.lst-kix_x4rtdbz82adu-4{list-style-type:none}.lst-kix_t6bn5dv180ed-0>li:before{content:"\0025cf   "}ul.lst-kix_x4rtdbz82adu-5{list-style-type:none}ul.lst-kix_x4rtdbz82adu-6{list-style-type:none}ul.lst-kix_s3obbdkamyme-8{list-style-type:none}ul.lst-kix_s3obbdkamyme-6{list-style-type:none}ul.lst-kix_s3obbdkamyme-7{list-style-type:none}ul.lst-kix_s3obbdkamyme-4{list-style-type:none}ul.lst-kix_s3obbdkamyme-5{list-style-type:none}ul.lst-kix_s3obbdkamyme-2{list-style-type:none}ul.lst-kix_s3obbdkamyme-3{list-style-type:none}ul.lst-kix_s3obbdkamyme-0{list-style-type:none}ul.lst-kix_s3obbdkamyme-1{list-style-type:none}.lst-kix_x4rtdbz82adu-1>li:before{content:"\0025cb   "}.lst-kix_x4rtdbz82adu-3>li:before{content:"\0025cf   "}ul.lst-kix_v0j4lzf1trgl-0{list-style-type:none}.lst-kix_x4rtdbz82adu-2>li:before{content:"\0025a0   "}.lst-kix_x4rtdbz82adu-6>li:before{content:"\0025cf   "}ul.lst-kix_t6bn5dv180ed-2{list-style-type:none}ul.lst-kix_t6bn5dv180ed-3{list-style-type:none}.lst-kix_v0j4lzf1trgl-8>li:before{content:"\0025a0   "}ul.lst-kix_t6bn5dv180ed-4{list-style-type:none}.lst-kix_x4rtdbz82adu-7>li:before{content:"\0025cb   "}ul.lst-kix_t6bn5dv180ed-5{list-style-type:none}ul.lst-kix_t6bn5dv180ed-6{list-style-type:none}ul.lst-kix_t6bn5dv180ed-7{list-style-type:none}ul.lst-kix_t6bn5dv180ed-8{list-style-type:none}.lst-kix_x4rtdbz82adu-0>li:before{content:"\0025cf   "}.lst-kix_x4rtdbz82adu-8>li:before{content:"\0025a0   "}.lst-kix_v0j4lzf1trgl-4>li:before{content:"\0025cb   "}.lst-kix_v0j4lzf1trgl-3>li:before{content:"\0025cf   "}ul.lst-kix_t6bn5dv180ed-0{list-style-type:none}ul.lst-kix_t6bn5dv180ed-1{list-style-type:none}.lst-kix_v0j4lzf1trgl-7>li:before{content:"\0025cb   "}.lst-kix_x4rtdbz82adu-5>li:before{content:"\0025a0   "}.lst-kix_v0j4lzf1trgl-5>li:before{content:"\0025a0   "}.lst-kix_v0j4lzf1trgl-6>li:before{content:"\0025cf   "}.lst-kix_x4rtdbz82adu-4>li:before{content:"\0025cb   "}.lst-kix_t6bn5dv180ed-2>li:before{content:"\0025a0   "}.lst-kix_t6bn5dv180ed-3>li:before{content:"\0025cf   "}.lst-kix_s3obbdkamyme-1>li:before{content:"\0025cb   "}.lst-kix_s3obbdkamyme-2>li:before{content:"\0025a0   "}.lst-kix_t6bn5dv180ed-4>li:before{content:"\0025cb   "}.lst-kix_s3obbdkamyme-4>li:before{content:"\0025cb   "}.lst-kix_v0j4lzf1trgl-0>li:before{content:"\0025cf   "}.lst-kix_t6bn5dv180ed-6>li:before{content:"\0025cf   "}.lst-kix_t6bn5dv180ed-7>li:before{content:"\0025cb   "}.lst-kix_s3obbdkamyme-3>li:before{content:"\0025cf   "}.lst-kix_s3obbdkamyme-5>li:before{content:"\0025a0   "}.lst-kix_v0j4lzf1trgl-1>li:before{content:"\0025cb   "}.lst-kix_v0j4lzf1trgl-2>li:before{content:"\0025a0   "}.lst-kix_t6bn5dv180ed-5>li:before{content:"\0025a0   "}ul.lst-kix_fw031qduy7rh-6{list-style-type:none}.lst-kix_s3obbdkamyme-8>li:before{content:"\0025a0   "}ul.lst-kix_fw031qduy7rh-7{list-style-type:none}ul.lst-kix_fw031qduy7rh-4{list-style-type:none}.lst-kix_s3obbdkamyme-7>li:before{content:"\0025cb   "}.lst-kix_fw031qduy7rh-2>li:before{content:"\0025a0   "}.lst-kix_fw031qduy7rh-4>li:before{content:"\0025cb   "}ul.lst-kix_fw031qduy7rh-5{list-style-type:none}ul.lst-kix_fw031qduy7rh-2{list-style-type:none}.lst-kix_s3obbdkamyme-6>li:before{content:"\0025cf   "}ul.lst-kix_fw031qduy7rh-3{list-style-type:none}.lst-kix_t6bn5dv180ed-8>li:before{content:"\0025a0   "}ul.lst-kix_fw031qduy7rh-0{list-style-type:none}ul.lst-kix_fw031qduy7rh-1{list-style-type:none}.lst-kix_fw031qduy7rh-3>li:before{content:"\0025cf   "}ul.lst-kix_v0j4lzf1trgl-3{list-style-type:none}ul.lst-kix_v0j4lzf1trgl-4{list-style-type:none}ul.lst-kix_v0j4lzf1trgl-1{list-style-type:none}.lst-kix_fw031qduy7rh-0>li:before{content:"\0025cf   "}ul.lst-kix_v0j4lzf1trgl-2{list-style-type:none}ul.lst-kix_v0j4lzf1trgl-7{list-style-type:none}ul.lst-kix_v0j4lzf1trgl-8{list-style-type:none}ul.lst-kix_v0j4lzf1trgl-5{list-style-type:none}.lst-kix_fw031qduy7rh-1>li:before{content:"\0025cb   "}ul.lst-kix_v0j4lzf1trgl-6{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c1{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c3{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c5{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c11{margin-left:36pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c14{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c4{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c8{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c2{padding:0;margin:0}.c9{color:inherit;text-decoration:inherit}.c12{margin-left:36pt}.c10{font-style:italic}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c8"><p class="c5 title" id="h.syd6hvhb4n9u"><span class="c13">MDP planning tutorial pointers</span></p><h1 class="c3" id="h.hw8wsftodhy0"><span class="c6">MDP background </span></h1><p class="c7"><span class="c0"></span></p><ul class="c2 lst-kix_fw031qduy7rh-0 start"><li class="c1"><span>Puterman, Martin L. 2005. </span><span class="c10">Markov Decision Processes (Discrete Stochastic Dynamic Programming)</span><span class="c0">. Wiley-Interscience.<br><br>General background on MDPs; many different criteria. Rigorous build-up of foundations.</span></li></ul><p class="c7"><span class="c0"></span></p><ul class="c2 lst-kix_x4rtdbz82adu-0 start"><li class="c1"><span>Kallenberg, Lodewijk. n.d. </span><span class="c10">MARKOV DECISION PROCESSES: Lecture Notes</span><span class="c0">. Available online.<br><br>Another general intro; variations. Occupancy measures. Some proofs are nice. A bit too many examples for my taste, but, hey, one can skip those..<br></span></li><li class="c1"><span>Arapostathis, Aristotle, Vivek S. Borkar, Emmanuel Fern&aacute;ndez-Gaucherand, Mrinal K. Ghosh, and Steven I. Marcus. 1993. &ldquo;Discrete-Time Controlled Markov Processes with Average Cost Criterion: A Survey.&rdquo; </span><span class="c10">SIAM Journal on Control and Optimization</span><span class="c0">&nbsp;31 (2): 282&ndash;344.<br><br>Survey of average-cost case<br></span></li><li class="c1"><span>Lattimore, Tor, and Csaba Szepesvari. 2020. Chapter 38 of </span><span class="c10">Bandit Algorithms</span><span>. Cambridge University Press. </span><span class="c4"><a class="c9" href="https://www.google.com/url?q=https://tor-lattimore.com/downloads/book/book.pdf&amp;sa=D&amp;ust=1600421625286000&amp;usg=AOvVaw0ybVqleGq9sghblAWUuliB">https://tor-lattimore.com/downloads/book/book.pdf</a></span><span class="c0"><br><br>The chapter rigorously builds up the theory for finite state-action space average reward MDPs with &ldquo;finite diameter&rdquo;. Short, self-contained. It rigorously argues for the sufficiency of deterministic stationary policies. The only catch is that quite a few of the proofs are left to the reader as exercises. If needed, the solutions are available from the authors. The proof uses the vanishing discount approach; so most of the tools that are needed to handle the discounted case are developed in this chapter.<br></span></li><li class="c1"><span>Altman, Eitan. 2009. </span><span class="c10">Constrained Markov Decision Processes</span><span class="c0">.<br><br>Classic on constrained MDPs. Structural results. Proof for sufficiency of stationary policies is from here.<br></span></li><li class="c1"><span>Brief into to CMDPs:<br></span><span class="c4"><a class="c9" href="https://www.google.com/url?q=http://readingsml.blogspot.com/2020/03/constrained-mdps-and-reward-hypothesis.html&amp;sa=D&amp;ust=1600421625287000&amp;usg=AOvVaw3PWsLz1fhcmAqfczNqnUPQ">http://readingsml.blogspot.com/2020/03/constrained-mdps-and-reward-hypothesis.html</a></span><span class="c0"><br><br>Sufficiency of stationary policies reproduced here. This also discusses in what ways the MDP framework where there is a single objective is potentially not sufficiently rich to capture many problems of practical interest. Interestingly, the algorithms and techniques developed for the MDP framework continue to be useful beyond MDPs.</span></li></ul><p class="c7"><span class="c0"></span></p><ul class="c2 lst-kix_x4rtdbz82adu-0"><li class="c1"><span>Bertsekas, Dimitri P. 2005. </span><span class="c10">Dynamic Programming and Optimal Control (Volumes 1,2)</span><span class="c0">. Athena Scientific.<br><br>Another rigorous introduction to MDPs; undiscounted, etc.<br></span></li><li class="c1"><span>Braziunas, Darius. n.d. &ldquo;POMDP Solution Methods.&rdquo; </span><span class="c4"><a class="c9" href="https://www.google.com/url?q=https://dariusb.bitbucket.io/papers/POMDP_survey.pdf&amp;sa=D&amp;ust=1600421625289000&amp;usg=AOvVaw1EEstyeB5U5mCvXiaGZ3hu">https://dariusb.bitbucket.io/papers/POMDP_survey.pdf</a></span><span class="c0">.<br><br>POMDP background; with a focus on some specific computational approaches.<br></span></li><li class="c1"><span>Mykel J. Kochenderfer&rsquo;s slides on MDPs and POMDPs. </span><span class="c4"><a class="c9" href="https://www.google.com/url?q=https://web.stanford.edu/~mykel/pomdps.pdf&amp;sa=D&amp;ust=1600421625289000&amp;usg=AOvVaw3HK8oZc2gPPanhIShfoswO">https://web.stanford.edu/~mykel/pomdps.pdf</a></span><span class="c0"><br><br>Comes with the book:<br></span></li><li class="c1"><span>Kochenderfer, Mykel J. </span><span class="c10">Decision Making Under Uncertainty: Theory and Application</span><span class="c0">. 1st ed. The MIT Press.<br><br>Gives an overview of various approaches; the coverage of POMDPs is quite nice. The emphasis is not on rigor or theory, but the book is meant to be easy to read and helpful to build intuition.<br></span></li><li class="c1"><span>&Aring;str&ouml;m, Karl Johan. 1965. &ldquo;Optimal Control of Markov Processes with Incomplete State Information&rdquo; </span><span class="c10">Journal of Mathematical Analysis and Applications</span><span class="c0">&nbsp;10: 174&ndash;205.</span></li></ul><p class="c7 c12"><span class="c0"></span></p><p class="c11"><span>The paper that introduced POMDPs(?).</span></p><h1 class="c3" id="h.a6t8rkmweow8"><span class="c6">Local planning; tree building/lookahead search</span></h1><ul class="c2 lst-kix_x4rtdbz82adu-0"><li class="c1"><span>Kearns, M., Y. Mansour, and A. Ng. 2002. &ldquo;A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes.&rdquo; </span><span class="c10">Machine Learning</span><span class="c0">&nbsp;49 (January): 193&ndash;208.<br><br>The journal version of the method that builds a tree of depth equal to the effective horizon. </span></li></ul><p class="c7 c12"><span class="c0"></span></p><ul class="c2 lst-kix_x4rtdbz82adu-0"><li class="c1"><span>Kearns, M., Y. Mansour, and A. Ng. 2000. &ldquo;Approximate Planning in Large POMDPs via Reusable Trajectories.&rdquo; In </span><span class="c10">Advances in Neural Information Processing Systems</span><span>. </span><span class="c4"><a class="c9" href="https://www.google.com/url?q=http://www.robotics.stanford.edu/~ang/papers/pomdp-long.ps&amp;sa=D&amp;ust=1600421625291000&amp;usg=AOvVaw1eEk5rAm9-RYXSivIJJYTz">http://www.robotics.stanford.edu/~ang/papers/pomdp-long.ps</a></span><span class="c0">.<br><br>A followup to the early, conference version of the above paper, that uses separate trees with a single successor state per action and proves the basic uniform convergence results. This gives us query-complexity bounds independent of the size of the state space, but exponential in the effective horizon.<br></span></li><li class="c1"><span>Munos, R&eacute;mi. 2014. &ldquo;From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning.&rdquo; </span><span class="c10">Foundations and Trends&reg; in Machine Learning</span><span class="c0">&nbsp;7 (1): 1&ndash;129.<br><br>Summarizes, in a short book form, how to be more clever in building these trees. The goal is to avoid the exponential dependence on the planning horizon, which can only be done when the MDP is &ldquo;nice&rdquo;. This led to a number of non-uniform query/computational complexity results. Remi also had amazing demonstrations for deterministic MDPs:)<br></span></li><li class="c1"><span>Lee, Wee S., Nan Rong, and David Hsu. 2008. &ldquo;What Makes Some POMDP Problems Easy to Approximate?&rdquo; In </span><span class="c10">Advances in Neural Information Processing Systems 20</span><span class="c0">, edited by J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis, 689&ndash;96. Curran Associates, Inc.<br><br>Since we touched POMDPs; &ldquo;classic&rdquo; overlooked(?) paper that defines a space whose covering number governs the hardness of computation in a POMDP. The algorithm is &ldquo;point-based value iteration&rdquo;: It is a close relative to Rust&rsquo;s algorithm; see below.<br></span></li><li class="c1"><span>Lim, Michael H., Claire J. Tomlin, and Zachary N. Sunberg. 2019. &ldquo;Sparse Tree Search Optimality Guarantees in POMDPs with Continuous Observation Spaces.&rdquo; </span><span class="c10">arXiv [cs.LG]</span><span>. arXiv. </span><span class="c4"><a class="c9" href="https://www.google.com/url?q=http://arxiv.org/abs/1910.04332&amp;sa=D&amp;ust=1600421625293000&amp;usg=AOvVaw2fV_Muq1R-6kVOeO0zeMYZ">http://arxiv.org/abs/1910.04332</a></span><span class="c0">.<br><br>Because continuous observation spaces are also real.</span></li></ul><h1 class="c3" id="h.nl45hwm1mkun"><span class="c6">Local planning in smooth MDPs</span></h1><p class="c7"><span class="c0"></span></p><ul class="c2 lst-kix_x4rtdbz82adu-0"><li class="c1"><span>Rust, John. 1997. &ldquo;Using Randomization to Break the Curse of Dimensionality.&rdquo; </span><span class="c10">Econometrica: Journal of the Econometric Society</span><span class="c0">&nbsp;65 (3): 487.<br><br>This effectively considers local planning in smooth MDPs; showing a separation between global and local planning. The Chow-Tsitsiklis paper proved a lower bound for the same class for global planning that scaled exponentially with the state-action space dimension.<br></span></li><li class="c1"><span>Szepesv&aacute;ri, C. 2001. &ldquo;Efficient Approximate Planning in Continuous Space Markovian Decision Problems.&rdquo; </span><span class="c10">AI Communications. The European Journal on Artificial Intelligence</span><span class="c0">&nbsp;14 (January): 163&ndash;76.<br><br>Since the previous paper had a result only bounding the value function estimation bias (which is at best an auxiliary question), there remained the question of whether we can actually use the method for efficient action-computation. This is answered positively here.</span></li></ul><p class="c7 c12"><span class="c0"></span></p><ul class="c2 lst-kix_x4rtdbz82adu-0"><li class="c1"><span>Chow, Chef-Seng, and John N. Tsitsiklis. 1989. &ldquo;The Complexity of Dynamic Programming.&rdquo; </span><span class="c10">Journal of Complexity</span><span class="c0">&nbsp;5 (4): 466&ndash;88.<br><br>The paper mentioned above. Lower bound for global planning. Only for value function approximation. Todo: Show the same bound hold for policy computation.<br></span></li></ul><h1 class="c3" id="h.ovchmoks7ka"><span class="c6">Policy search</span></h1><ul class="c2 lst-kix_x4rtdbz82adu-0"><li class="c1"><span>Vlassis, Nikos, Michael L. Littman, and David Barber. 2012. &ldquo;On the Computational Complexity of Stochastic Controller Optimization in POMDPs.&rdquo; </span><span class="c10">ACM Trans. Comput. Theory</span><span class="c0">, 12, 4 (4): 1&ndash;8.<br><br>One can use a result from this paper to show that policy search for constant policies (they choose the action from the same probability distribution, no matter the state), is NP-hard. <br></span></li><li class="c1"><span>Hansen, E. 1998. &ldquo;Solving POMDPs by Searching in Policy Space.&rdquo; In </span><span class="c10">Proceedings of the Fourteenth Conference on &hellip;</span><span>. </span><span class="c4"><a class="c9" href="https://www.google.com/url?q=http://dl.acm.org/citation.cfm?id%3D2074119&amp;sa=D&amp;ust=1600421625295000&amp;usg=AOvVaw0P0kYpR33u8mh-ikiMKSPl">http://dl.acm.org/citation.cfm?id=2074119</a></span><span class="c0">.<br><br>Gives basic discretization results. I did not talk about this.<br></span></li><li class="c1"><span>Ng, Andrew Y., and Michael Jordan. 2000. &ldquo;PEGASUS: A Policy Search Method for Large MDPs and POMDPs.&rdquo; In </span><span class="c10">Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence</span><span class="c0">, 406&ndash;15. UAI&rsquo;00. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.<br><br>Another paper that I did not talk about. This paper considers what is known as the scenario-method in operations research. The question in a way is whether the stochastic search problems are much harder or not than the deterministic problems. The scenario method assumes a simulator with controlled random seeds; the idea is to take finitely many such seeds and approximate the value of a policy using the average across the seeds. Uniform convergence results are shown; smoothness is needed for infinitely many actions.</span></li></ul><p class="c7"><span class="c0"></span></p><ul class="c2 lst-kix_x4rtdbz82adu-0"><li class="c1"><span>Bagnell, J., S. Kakade, A. Ng, and J. Schneider. 2004. &ldquo;Policy Search by Dynamic Programming.&rdquo; In </span><span class="c10">Advances in Neural Information Processing Systems</span><span>. </span><span class="c4"><a class="c9" href="https://www.google.com/url?q=http://papers.nips.cc/paper/2378-policy-search-by-dynamic-programming&amp;sa=D&amp;ust=1600421625296000&amp;usg=AOvVaw1ia_xghAQU5jov40vUcsTy">http://papers.nips.cc/paper/2378-policy-search-by-dynamic-programming</a></span><span class="c0">.<br><br>Another paper I had to skip. Finite horizon MDPs. Algorithm is given a distribution over states for each round. Goal is to reduce policy search to &ldquo;simpler&rdquo; search. It works backwards, finding the policy for round t that maximizes value starting from the said distribution and for the rest of rounds using policies found previously. (Additive) error bound on how well the (nonstationary) policy will do as a function of how much the distributions are mismatched to that of the optimal policy and the accuracy of finding the best policy in each round. </span></li></ul><h1 class="c3" id="h.v3c08y1yog8k"><span class="c6">Policy gradients</span></h1><p class="c7"><span class="c0"></span></p><ul class="c2 lst-kix_x4rtdbz82adu-0"><li class="c1"><span>Williams, Ronald J. 1992. &ldquo;Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning.&rdquo; </span><span class="c10">Machine Learning</span><span class="c0">&nbsp;8 (3-4): 229&ndash;56.<br><br>Reinforce algorithm; score function method appearing in ML.<br></span></li><li class="c1"><span>Glynn, Peter W. 1990. &ldquo;Likelihood Ratio Gradient Estimation for Stochastic Systems.&rdquo; </span><span class="c10">Communications of the ACM</span><span class="c0">&nbsp;33 (10): 75&ndash;84.<br><br>Same from OR.<br></span></li><li class="c1"><span>Sutton, R., D. McAllester, S. Singh, and Y. Mansour. 2000. &ldquo;Policy Gradient Methods for Reinforcement Learning with Function Approximation.&rdquo; In </span><span class="c10">NIPS-12</span><span class="c0">, 1057&ndash;63.<br><br>I did not talk about this; alternative gradient expression. The paper also introduces the notion of compatible function approximation. I did not talk about this either.</span></li></ul><p class="c7"><span class="c0"></span></p><h1 class="c3" id="h.rssfr2tl2rgj"><span class="c6">Global convergence of policy gradients</span></h1><p class="c7"><span class="c0"></span></p><p class="c14"><span class="c0">Check bibliography in these papers and citations! New papers every day!</span></p><p class="c7"><span class="c0"></span></p><ul class="c2 lst-kix_s3obbdkamyme-0 start"><li class="c1"><span>Bhandari, Jalaj, and Daniel Russo. 2019. &ldquo;Global Optimality Guarantees For Policy Gradient Methods,&rdquo; June. </span><span class="c4"><a class="c9" href="https://www.google.com/url?q=https://arxiv.org/abs/1906.01786v1&amp;sa=D&amp;ust=1600421625299000&amp;usg=AOvVaw3r1m8g-kygkWgZhwQhWOcX">https://arxiv.org/abs/1906.01786v1</a></span><span class="c0">.</span></li><li class="c1"><span>Bhandari, Jalaj, and Daniel Russo. 2020. &ldquo;A Note on the Linear Convergence of Policy Gradient Methods.&rdquo; </span><span class="c10">arXiv [cs.LG]</span><span>. arXiv. </span><span class="c4"><a class="c9" href="https://www.google.com/url?q=http://arxiv.org/abs/2007.11120&amp;sa=D&amp;ust=1600421625300000&amp;usg=AOvVaw3H-6nkl8Sl-UJrMgy7AYcU">http://arxiv.org/abs/2007.11120</a></span></li><li class="c1"><span class="c0">Fazel, Maryam, Rong Ge, Sham Kakade, and Mehran Mesbahi. 2018. &ldquo;Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator.&rdquo; Edited by Jennifer Dy and Andreas Krause, Proceedings of Machine Learning Research, 80: 1467&ndash;76.</span></li><li class="c1"><span>Agarwal, Alekh, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. 2019. &ldquo;Optimality and Approximation with Policy Gradient Methods in Markov Decision Processes,&rdquo; August. </span><span class="c4"><a class="c9" href="https://www.google.com/url?q=https://arxiv.org/abs/1908.00261v2&amp;sa=D&amp;ust=1600421625301000&amp;usg=AOvVaw0fK2qzK23jEMdUwU965Rym">https://arxiv.org/abs/1908.00261v2</a></span><span class="c0">.</span></li><li class="c1"><span>Zhang, Junyu, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang. 2020. &ldquo;Variational Policy Gradient Method for Reinforcement Learning with General Utilities.&rdquo; </span><span class="c10">arXiv [cs.LG]</span><span>. arXiv. </span><span class="c4"><a class="c9" href="https://www.google.com/url?q=http://arxiv.org/abs/2007.02151&amp;sa=D&amp;ust=1600421625301000&amp;usg=AOvVaw3GtfANztiw5Rl87GdnxA_I">http://arxiv.org/abs/2007.02151</a></span><span class="c0">.</span></li><li class="c1"><span>Mei, Jincheng, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. 2020. &ldquo;On the Global Convergence Rates of Softmax Policy Gradient Methods.&rdquo; </span><span class="c10">arXiv [cs.LG]</span><span>. arXiv. </span><span class="c4"><a class="c9" href="https://www.google.com/url?q=http://arxiv.org/abs/2005.06392&amp;sa=D&amp;ust=1600421625302000&amp;usg=AOvVaw3JlkcqxwkCh6w582Fcm232">http://arxiv.org/abs/2005.06392</a></span><span class="c0">.<br></span></li></ul><h1 class="c3" id="h.o46pd4jge0xi"><span class="c6">Value function approximation</span></h1><ul class="c2 lst-kix_v0j4lzf1trgl-0 start"><li class="c1"><span>Whitt, Ward. 1978. &ldquo;Approximations of Dynamic Programs, I.&rdquo; </span><span class="c10">Mathematics of Operations Research</span><span class="c0">&nbsp;3 (3): 231&ndash;43.<br><br>Classic paper on approximate dynamic programming. I did not cover any of this, but calculations similar to those in this paper come up all the time. The paper is about getting performance guarantees for the optimal policy of MDP A when the policy is used in MDP B. The goal is to show that the optimal policy for A is not bad in B when A and B are in a way close to each other. A bit hard to read because of the archaic notation, but it&#39;s totally worth reading it. Check out Theorem 3.1!<br></span></li><li class="c1"><span>Schweitzer, Paul J., and Abraham Seidmann. 1985. &ldquo;Generalized Polynomial Approximations in Markovian Decision Processes.&rdquo; </span><span class="c10">Journal of Mathematical Analysis and Applications</span><span class="c0">&nbsp;110 (2): 568&ndash;82.<br><br>The paper that explicitly introduces linear value function approximation in the context of planning in MDPs. No theoretical results, just ideas. The paper introduces essentially least-squares policy iteration (LSPI, no Q-functions though), LSVI (least squares value iteration), and ALP (approximate linear programming).<br></span></li><li class="c1"><span>Jin, Chi, Zhuoran Yang, Zhaoran Wang, and Michael I. Jordan. 2019. &ldquo;Provably Efficient Reinforcement Learning with Linear Function Approximation,&rdquo; July. </span><span class="c4"><a class="c9" href="https://www.google.com/url?q=https://arxiv.org/abs/1907.05388v2&amp;sa=D&amp;ust=1600421625303000&amp;usg=AOvVaw2VLBErXe_Z7s658tyinYQP">https://arxiv.org/abs/1907.05388v2</a></span><span class="c0">.<br><br>This is the paper that introduces linear MDPs (as discussed in the lecture; some other authors mean other things by linear MDPs). This is done in the context of exploration (exploration will be covered later). Earlier papers made weaker assumptions, like small/zero inherent Bellman error of the function space considered under all policies. Nevertheless, it is a nice discovery that this class of MDPs is ideal for policy iteration with linear function approximation. In particular, in linear MDPs the action-value function of any policy lies in the linear space spanned by the features, i.e., the action-value functions are all realizable.<br></span></li><li class="c1"><span>Du, Simon S., Sham M. Kakade, Ruosong Wang, and Lin F. Yang. 2019. &ldquo;Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?&rdquo; </span><span class="c10">arXiv [cs.LG]</span><span>. arXiv. </span><span class="c4"><a class="c9" href="https://www.google.com/url?q=http://arxiv.org/abs/1910.03016&amp;sa=D&amp;ust=1600421625304000&amp;usg=AOvVaw2UqlAuWLTQAgJfwjKwX-hq">http://arxiv.org/abs/1910.03016</a></span><span class="c0">.<br><br>This is the paper that shows that a slight departure from the realizability assumption (that is that all Q^pi lie in the linear span of features) can cause trouble: The planner may need to issue as many queries as the number of states even if the realizability assumption is just slightly violated.</span></li></ul><p class="c7 c12"><span class="c0"></span></p><ul class="c2 lst-kix_v0j4lzf1trgl-0"><li class="c1"><span>Lattimore, Tor, Csaba Szepesvari, and Gellert Weisz. 2019. &ldquo;Learning with Good Feature Representations in Bandits and in RL with a Generative Model.&rdquo; </span><span class="c10">arXiv [stat.ML]</span><span>. arXiv. </span><span class="c4"><a class="c9" href="https://www.google.com/url?q=http://arxiv.org/abs/1911.07676&amp;sa=D&amp;ust=1600421625305000&amp;usg=AOvVaw23LFU852-5-lvWkx3sxSIr">http://arxiv.org/abs/1911.07676</a></span><span class="c0">.<br><br>This paper refined the argument of the previous one: The moral of the story is that a precision that is as good as the realizability error is what ruins query complexity. If we allow the representation error to blow up by a factor related to the number of features, the blow-up disappears. The paper also introduces other refinements, such as that not all feature-maps require the same error-relaxation to avoid the query complexity blow-up (e.g., state aggregation definitely won&rsquo;t need this). Another refinement is the treatment of exploration in unrealizable linear bandits.<br></span></li><li class="c1"><span>Shariff, Roshan, and Csaba Szepesv&aacute;ri. 2020. &ldquo;Efficient Planning in Large MDPs with Weak Linear Function Approximation.&rdquo; </span><span class="c10">arXiv [cs.LG]</span><span>. arXiv. </span><span class="c4"><a class="c9" href="https://www.google.com/url?q=http://arxiv.org/abs/2007.06184&amp;sa=D&amp;ust=1600421625306000&amp;usg=AOvVaw2rr9fN9MBHrlxIQ1pUceVE">http://arxiv.org/abs/2007.06184</a></span><span class="c0">.<br><br>While I did not talk about this paper, it is highly relevant: The paper considers linear functions approximation, but aims to address the case when the only assumption made is that Q^* is close to the span of features. At the end, a bit more is assumed, access to a set of &ldquo;core&rdquo; states such that the convex hull of the features at these states includes the features of all other states. An ALP approach is followed and a fully polynomial time algorithm is derived, although the runtime depends on the number of core states (in addition to the number of features), though it is independent of the cardinality of the state-space.</span></li></ul><h1 class="c3" id="h.cbpd5qnch7my"><span class="c6">Other notable sources</span></h1><ul class="c2 lst-kix_t6bn5dv180ed-0 start"><li class="c1"><span>Agarwal, Alekh, Nan Jiang, and Sham M. Kakade. 2019. </span><span class="c10">Reinforcement Learning: Theory and Algorithms</span><span>. </span><span class="c4"><a class="c9" href="https://www.google.com/url?q=https://rltheorybook.github.io/rl_monograph_AJK.pdf&amp;sa=D&amp;ust=1600421625307000&amp;usg=AOvVaw3OhoScT2ruw1PeKC5IjFy3">https://rltheorybook.github.io/rl_monograph_AJK.pdf</a></span><span class="c0"><br><br>Course notes. Covers a whole spectrum of things.<br></span></li><li class="c1"><span>Mausam, and Andrey Kolobov. 2012. &ldquo;Planning with Markov Decision Processes: An AI Perspective.&rdquo; </span><span class="c10">Synthesis Lectures on Artificial Intelligence and Machine Learning</span><span class="c0">&nbsp;6 (1): 1&ndash;210.<br><br>Connection to heuristic search, determinization, connection to the classical planning literature; stochastic shortest path is really at the center (dead-ends and whatnot). Just missing theoretical results beyond the basics.<br></span></li></ul></body></html>